{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import numpy as np \nimport pandas as pd \nimport os\nimport re\nimport nltk",
      "metadata": {
        "trusted": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from nltk.corpus import stopwords\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers import Flatten\nfrom keras.layers import GlobalMaxPooling1D,Conv1D,LSTM\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom random import shuffle\nfrom tensorflow.python.keras import models\nfrom tensorflow.python.keras.layers import Dense\nfrom tensorflow.python.keras.layers import Dropout\nfrom keras import optimizers\nfrom keras.models import *\nfrom keras.layers import *",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#checking for null values\ndataset = pd.read_csv(\"./IMDB Dataset.csv\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "dataset.isnull().values.any()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "dataset.shape",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "dataset['review'][3]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from sklearn.feature_extraction.text import CountVectorizer\nimport matplotlib.pyplot as plt\nvectorizer = CountVectorizer()\nvect_texts = vectorizer.fit_transform(list(dataset['review']))\n\nall_ngrams = vectorizer.get_feature_names()\nnum_ngrams = min(50, len(all_ngrams))\nall_counts = vect_texts.sum(axis=0).tolist()[0]\n\nall_ngrams, all_counts = zip(*[(n, c) for c, n in sorted(zip(all_counts, all_ngrams), reverse=True)])\nngrams = all_ngrams[:num_ngrams]\ncounts = all_counts[:num_ngrams]\nidx = np.arange(num_ngrams)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "plt.figure(figsize=(30, 30))\nplt.bar(idx, counts, width=0.8)\nplt.xlabel('N-grams')\nplt.ylabel('Frequencies')\nplt.title('Frequency distribution of ngrams')\nplt.xticks(idx, ngrams, rotation=45)\nplt.show()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#pre processing reviews\nTAG_RE = re.compile(r'<[^>]+>')\ndef remove_tags(text):\n    return TAG_RE.sub('', text)\n\ndef preprocessing_text(text):\n    sentence = remove_tags(text)\n    sentence = re.sub(r'https:\\/\\/[a-zA-Z]*\\.com',' ',sentence)\n    sentence = re.sub(r'\\d+',' ',sentence)\n    sentence = re.sub(r'\\s+',' ',sentence)\n    sentence = re.sub(r\"\\b[a-zA-Z]\\b\", ' ', sentence)\n    sentence = re.sub(r'\\W+',' ',sentence)\n    sentence = sentence.lower()\n    return sentence",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#removing html tags \npre_proces_sen = []\nsentences = list(dataset['review'])\nfor sen in sentences:\n    pre_proces_sen.append(preprocessing_text(sen))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print(pre_proces_sen[3])",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "stop = ['has', 'its', \"needn't\", 'm', \"wouldn't\", 'but', 'he', \"mustn't\", 'his', 'there', 'or', \"won't\", 'can',\n        'd', \"hadn't\", 'how', 'hasn', 'very', 'wouldn', 'own', \"doesn't\", 'their', \"isn't\", 'an', \"haven't\",\n        \"wasn't\", 'those', 'once', \"shan't\", 'when', \"aren't\", 've', 'it', \"it's\", 'of', \"don't\", 'and', 'down', \n        'yours', 'to', 'over', \"she's\", 'we', 'they', 'haven', 'having', 'ain', 'no', 'her', 'you', 'then', 'just',\n        'didn', 'into', 'before', 'shouldn', 'here', 'yourselves', 's', 'will', 'which', 'are', 'who', 'with', \"you'd\", \n        'this', 'me', 'themselves', \"you've\", 'hadn', 'mightn', 'she', 'o', 'more', 'whom', 'for', 'him', 'again', 'below', \n        'few', 'most', 'been', 'such', 'shan', 'is', 'ourselves', 'y', 'by', 'being', 'in', 'mustn', \"you'll\", 'herself',\n        'yourself', 'ours', 'between', 'had', 'other', \"should've\", 't', 'isn', 'them', 'himself', 're', 'doing', 'only',\n        'where', 'your', 'after', 'so', 'll', 'against', 'the', 'about', 'each', 'aren', 'wasn', \"couldn't\", 'have', 'ma',\n        'i', 'my', \"mightn't\", 'as', 'from', 'itself', 'under', 'same', 'why', 'any', 'our', 'be', 'off', \"hasn't\", 'through',\n        \"you're\", 'was', 'did', \"shouldn't\", 'myself', 'some', 'theirs', 'hers', 'further', 'do', 'now', 'than', 'too', \n        'during', 'at', 'because', 'doesn','needn', \"weren't\", 'don', \"didn't\", 'couldn', 'what', 'does', 'if', 'up',\n        'on', 'these', 'should', 'all', \"that'll\", 'above', 'weren', 'that', 'a', 'while', 'both', 'until', 'were', 'am']",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "nltk.download('punkt')\nfor i in range(len(pre_proces_sen)):\n    x = pre_proces_sen[i] \n    x = word_tokenize(x)\n    new_x_list = [word for word in x if word not in stop]\n    pre_proces_sen[i] = ' '.join(new_x_list)\n    if i% 2000 == 0:\n        print(i,end=\" \")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print(pre_proces_sen[3])",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "y  = dataset['sentiment']\ny = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, y)))\n\nX = pre_proces_sen",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#training  and testing  sets\nX_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.50, random_state=42) \n\nX_val,X_test,y_val,y_test = train_test_split(X_tmp, y_tmp, test_size=0.40, random_state=42)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "tokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\n#creating sequence of tokenized words\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\nX_val = tokenizer.texts_to_sequences(X_val)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "checkpoint_dir = my_temp_folder+'training_checkpoints'\nimport shutil\ntry:\n    shutil.rmtree(checkpoint_dir)\nexcept:\n    print(\"directory not used yet.\")\n\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_\")\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    monitor='loss',\n    save_weights_only=True, \n    save_best_only=True)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#Models\n#using rnn",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "rnn_units = 128\nBATCH_SIZE = 256\ndropout_rate = 0.2",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Embedding(vocab_size, 300,input_length=maxlen))\nmodel.add(tf.keras.layers.SimpleRNN(rnn_units))\nmodel.add(tf.keras.layers.Dropout(rate=dropout_rate))\nmodel.add(tf.keras.layers.Dense(64,activation = 'relu'))\nmodel.add(tf.keras.layers.Dropout(rate=dropout_rate))\nmodel.add(tf.keras.layers.Dense(1,activation = 'sigmoid'))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "model.summary()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}